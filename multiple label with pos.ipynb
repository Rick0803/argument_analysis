{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "\n",
    "import collections\n",
    "import functools as fts\n",
    "import itertools as its\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "from project import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified and grouped training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = load_dataset(\"dataset.csv\")\n",
    "AGGREGATED = pd.read_csv(\"aggregated.csv\", index_col=0, converters={\"Document\": json.loads})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.merge(DATASET, AGGREGATED[[\"ID\", \"Document\"]], on = \"ID\", how=\"left\")\n",
    "DATASET = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_list(document: dict) -> list[str]:\n",
    "    return [word[\"upos\"] for sentence in document for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26068/3149995989.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  DATASET.loc[:, \"POS\"] = DATASET[\"Document\"].apply(get_pos_list)\n"
     ]
    }
   ],
   "source": [
    "DATASET.loc[:, \"POS\"] = DATASET[\"Document\"].apply(get_pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Issue</th>\n",
       "      <th>Stance</th>\n",
       "      <th>Argument</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>Argumentative</th>\n",
       "      <th>CO</th>\n",
       "      <th>LA</th>\n",
       "      <th>LR</th>\n",
       "      <th>LS</th>\n",
       "      <th>...</th>\n",
       "      <th>CL</th>\n",
       "      <th>AP</th>\n",
       "      <th>AR</th>\n",
       "      <th>RE</th>\n",
       "      <th>GA</th>\n",
       "      <th>GR</th>\n",
       "      <th>GS</th>\n",
       "      <th>OV</th>\n",
       "      <th>Document</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arg219250</td>\n",
       "      <td>ban-plastic-water-bottles</td>\n",
       "      <td>no-bad-for-the-economy</td>\n",
       "      <td>it is true that bottled water is a waste, but ...</td>\n",
       "      <td>1</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[{'id': 1, 'text': 'it', 'lemma': 'it', 'upos...</td>\n",
       "      <td>[PRON, AUX, ADJ, SCONJ, VERB, NOUN, AUX, DET, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>arg219250</td>\n",
       "      <td>ban-plastic-water-bottles</td>\n",
       "      <td>no-bad-for-the-economy</td>\n",
       "      <td>it is true that bottled water is a waste, but ...</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[{'id': 1, 'text': 'it', 'lemma': 'it', 'upos...</td>\n",
       "      <td>[PRON, AUX, ADJ, SCONJ, VERB, NOUN, AUX, DET, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>arg219250</td>\n",
       "      <td>ban-plastic-water-bottles</td>\n",
       "      <td>no-bad-for-the-economy</td>\n",
       "      <td>it is true that bottled water is a waste, but ...</td>\n",
       "      <td>3</td>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'it', 'lemma': 'it', 'upos...</td>\n",
       "      <td>[PRON, AUX, ADJ, SCONJ, VERB, NOUN, AUX, DET, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arg219293</td>\n",
       "      <td>ban-plastic-water-bottles</td>\n",
       "      <td>no-bad-for-the-economy</td>\n",
       "      <td>Most Americans on average recycle 86-88% of th...</td>\n",
       "      <td>1</td>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'Most', 'lemma': 'most', '...</td>\n",
       "      <td>[ADJ, PROPN, ADP, NOUN, VERB, NUM, SYM, NUM, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>arg219293</td>\n",
       "      <td>ban-plastic-water-bottles</td>\n",
       "      <td>no-bad-for-the-economy</td>\n",
       "      <td>Most Americans on average recycle 86-88% of th...</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[{'id': 1, 'text': 'Most', 'lemma': 'most', '...</td>\n",
       "      <td>[ADJ, PROPN, ADP, NOUN, VERB, NUM, SYM, NUM, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>arg168822</td>\n",
       "      <td>william-farquhar-ought-to-be-honoured-as-the-r...</td>\n",
       "      <td>yes-of-course</td>\n",
       "      <td>Raffles neglected Singapore when he went aroun...</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...</td>\n",
       "      <td>[PROPN, VERB, PROPN, ADV, PRON, VERB, SCONJ, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>arg168822</td>\n",
       "      <td>william-farquhar-ought-to-be-honoured-as-the-r...</td>\n",
       "      <td>yes-of-course</td>\n",
       "      <td>Raffles neglected Singapore when he went aroun...</td>\n",
       "      <td>3</td>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...</td>\n",
       "      <td>[PROPN, VERB, PROPN, ADV, PRON, VERB, SCONJ, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>arg168834</td>\n",
       "      <td>william-farquhar-ought-to-be-honoured-as-the-r...</td>\n",
       "      <td>yes-of-course</td>\n",
       "      <td>Raffles doesn't care about the citizens, doesn...</td>\n",
       "      <td>1</td>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...</td>\n",
       "      <td>[PROPN, AUX, PART, VERB, ADP, DET, NOUN, PUNCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>arg168834</td>\n",
       "      <td>william-farquhar-ought-to-be-honoured-as-the-r...</td>\n",
       "      <td>yes-of-course</td>\n",
       "      <td>Raffles doesn't care about the citizens, doesn...</td>\n",
       "      <td>2</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...</td>\n",
       "      <td>[PROPN, AUX, PART, VERB, ADP, DET, NOUN, PUNCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>arg168834</td>\n",
       "      <td>william-farquhar-ought-to-be-honoured-as-the-r...</td>\n",
       "      <td>yes-of-course</td>\n",
       "      <td>Raffles doesn't care about the citizens, doesn...</td>\n",
       "      <td>3</td>\n",
       "      <td>y</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...</td>\n",
       "      <td>[PROPN, AUX, PART, VERB, ADP, DET, NOUN, PUNCT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>909 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                              Issue  \\\n",
       "0    arg219250                          ban-plastic-water-bottles   \n",
       "1    arg219250                          ban-plastic-water-bottles   \n",
       "2    arg219250                          ban-plastic-water-bottles   \n",
       "3    arg219293                          ban-plastic-water-bottles   \n",
       "4    arg219293                          ban-plastic-water-bottles   \n",
       "..         ...                                                ...   \n",
       "929  arg168822  william-farquhar-ought-to-be-honoured-as-the-r...   \n",
       "930  arg168822  william-farquhar-ought-to-be-honoured-as-the-r...   \n",
       "931  arg168834  william-farquhar-ought-to-be-honoured-as-the-r...   \n",
       "932  arg168834  william-farquhar-ought-to-be-honoured-as-the-r...   \n",
       "933  arg168834  william-farquhar-ought-to-be-honoured-as-the-r...   \n",
       "\n",
       "                     Stance  \\\n",
       "0    no-bad-for-the-economy   \n",
       "1    no-bad-for-the-economy   \n",
       "2    no-bad-for-the-economy   \n",
       "3    no-bad-for-the-economy   \n",
       "4    no-bad-for-the-economy   \n",
       "..                      ...   \n",
       "929           yes-of-course   \n",
       "930           yes-of-course   \n",
       "931           yes-of-course   \n",
       "932           yes-of-course   \n",
       "933           yes-of-course   \n",
       "\n",
       "                                              Argument  Annotator  \\\n",
       "0    it is true that bottled water is a waste, but ...          1   \n",
       "1    it is true that bottled water is a waste, but ...          2   \n",
       "2    it is true that bottled water is a waste, but ...          3   \n",
       "3    Most Americans on average recycle 86-88% of th...          1   \n",
       "4    Most Americans on average recycle 86-88% of th...          2   \n",
       "..                                                 ...        ...   \n",
       "929  Raffles neglected Singapore when he went aroun...          2   \n",
       "930  Raffles neglected Singapore when he went aroun...          3   \n",
       "931  Raffles doesn't care about the citizens, doesn...          1   \n",
       "932  Raffles doesn't care about the citizens, doesn...          2   \n",
       "933  Raffles doesn't care about the citizens, doesn...          3   \n",
       "\n",
       "    Argumentative  CO  LA  LR  LS  ...  CL  AP  AR  RE  GA  GR  GS  OV  \\\n",
       "0               y   1   1   1   1  ...   2   1   1   1   1   1   1   1   \n",
       "1               y   1   3   2   1  ...   3   2   2   2   3   1   1   1   \n",
       "2               y   2   2   3   2  ...   2   2   2   2   2   2   2   2   \n",
       "3               y   2   3   3   2  ...   2   2   2   2   2   3   2   2   \n",
       "4               y   1   2   2   1  ...   2   1   2   1   2   1   1   1   \n",
       "..            ...  ..  ..  ..  ..  ...  ..  ..  ..  ..  ..  ..  ..  ..   \n",
       "929             y   1   2   2   1  ...   2   3   2   2   3   2   1   2   \n",
       "930             y   2   2   3   2  ...   2   2   2   2   2   3   2   2   \n",
       "931             y   2   2   3   2  ...   1   2   2   2   2   3   2   2   \n",
       "932             y   1   2   2   1  ...   3   3   2   2   2   2   1   2   \n",
       "933             y   2   2   3   2  ...   2   2   2   2   2   3   2   2   \n",
       "\n",
       "                                              Document  \\\n",
       "0    [[{'id': 1, 'text': 'it', 'lemma': 'it', 'upos...   \n",
       "1    [[{'id': 1, 'text': 'it', 'lemma': 'it', 'upos...   \n",
       "2    [[{'id': 1, 'text': 'it', 'lemma': 'it', 'upos...   \n",
       "3    [[{'id': 1, 'text': 'Most', 'lemma': 'most', '...   \n",
       "4    [[{'id': 1, 'text': 'Most', 'lemma': 'most', '...   \n",
       "..                                                 ...   \n",
       "929  [[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...   \n",
       "930  [[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...   \n",
       "931  [[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...   \n",
       "932  [[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...   \n",
       "933  [[{'id': 1, 'text': 'Raffles', 'lemma': 'Raffl...   \n",
       "\n",
       "                                                   POS  \n",
       "0    [PRON, AUX, ADJ, SCONJ, VERB, NOUN, AUX, DET, ...  \n",
       "1    [PRON, AUX, ADJ, SCONJ, VERB, NOUN, AUX, DET, ...  \n",
       "2    [PRON, AUX, ADJ, SCONJ, VERB, NOUN, AUX, DET, ...  \n",
       "3    [ADJ, PROPN, ADP, NOUN, VERB, NUM, SYM, NUM, S...  \n",
       "4    [ADJ, PROPN, ADP, NOUN, VERB, NUM, SYM, NUM, S...  \n",
       "..                                                 ...  \n",
       "929  [PROPN, VERB, PROPN, ADV, PRON, VERB, SCONJ, V...  \n",
       "930  [PROPN, VERB, PROPN, ADV, PRON, VERB, SCONJ, V...  \n",
       "931  [PROPN, AUX, PART, VERB, ADP, DET, NOUN, PUNCT...  \n",
       "932  [PROPN, AUX, PART, VERB, ADP, DET, NOUN, PUNCT...  \n",
       "933  [PROPN, AUX, PART, VERB, ADP, DET, NOUN, PUNCT...  \n",
       "\n",
       "[909 rows x 23 columns]"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rick/.anaconda/envs/rick/lib/python3.10/site-packages/sklearn/model_selection/_split.py:909: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "identifier = 2**DATASET['CO'] * 3**DATASET['EF'] * 5**DATASET['RE']\n",
    "train, test = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=300).split(DATASET, identifier, DATASET[\"ID\"]).__next__()\n",
    "\n",
    "TRAIN = DATASET.iloc[train].reset_index(drop=True)\n",
    "TEST = DATASET.iloc[test].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Vocabulary and the Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11000"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = set(DATASET[\"Argument\"])\n",
    "tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(\" \".join(corpus).lower())\n",
    "\n",
    "tokens = [token for token in tokens if token not in stopwords]\n",
    "\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_stance = set(DATASET['Stance'])\n",
    "    \n",
    "tokens_stance = tokenizer.tokenize(\" \".join(i.replace(\"-\", \" \") for i in corpus_stance).lower())\n",
    "\n",
    "tokens_stance = [token for token in tokens_stance if token not in stopwords]\n",
    "\n",
    "tokens.extend(tokens_stance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11051"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted({token for token, freq in collections.Counter(tokens).most_common(500)})\n",
    "vocab = dict(enumerate(vocab, start=2))\n",
    "vocab[0] = \"<pad>\"\n",
    "vocab[1] = \"<unk>\"\n",
    "vocab[2] = \"ADJ\"\n",
    "vocab[3] = \"ADV\"\n",
    "vocab[4] = \"INTJ\"\n",
    "vocab[5] = \"NOUN\"\n",
    "vocab[6] = \"PROPN\"\n",
    "vocab[7] = \"VERB\"\n",
    "vocab[8] = \"ADP\"\n",
    "vocab[9] = \"AUX\"\n",
    "vocab[10] = \"CCONJ\"\n",
    "vocab[11] = \"DET\"\n",
    "vocab[12] = \"NUM\"\n",
    "vocab[13] = \"PART\"\n",
    "vocab[14] = \"PRON\"\n",
    "vocab[15] = \"SCONJ\"\n",
    "vocab[16] = \"PUNCT\"\n",
    "vocab[17] = \"SYM\"\n",
    "vocab[18] = \"X\"\n",
    "\n",
    "vocab = {tok: idx for idx, tok in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens: list[str]) -> list[int]:\n",
    "    return [vocab.get(token, 1) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QualityDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, dimension: list) -> None:\n",
    "        self.dataset = [] # (ids: list[int], label: list)\n",
    "        \n",
    "        for _, instance in dataframe.iterrows():\n",
    "            tokens = tokenizer.tokenize(instance[\"Argument\"])\n",
    "            ids = vectorize(tokens)+[0]\n",
    "            tokens_stance = tokenizer.tokenize(instance[\"Stance\"])\n",
    "            ids_stance = vectorize(tokens_stance)\n",
    "            pos = vectorize((instance[\"POS\"]))\n",
    "            label = []\n",
    "            for i in range(len(dimension)):\n",
    "                label.append(instance[dimension[i]] - 1)\n",
    "            \n",
    "            self.dataset.append((torch.cat((torch.tensor(ids, dtype=torch.int, device=\"cuda\"), torch.tensor(ids_stance, dtype=torch.int, device=\"cuda\"), \n",
    "                                            torch.tensor(pos, dtype=torch.int, device=\"cuda\"))), label))\n",
    "            \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> tuple[torch.tensor, int]:\n",
    "        return self.dataset[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATASET = QualityDataset(TRAIN, [\"OV\", \"CO\", \"EF\", \"RE\"])\n",
    "TEST_DATASET  = QualityDataset(TEST, [\"OV\", \"CO\", \"EF\", \"RE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for X, Y in TRAIN_DATASET:\n",
    "    #print(X)\n",
    "    #print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineGRU(nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, gru_hidden_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.gru_hidden_dim = gru_hidden_dim\n",
    "        \n",
    "        self.embedder = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.encoder = nn.GRU(embedding_dim, gru_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(gru_hidden_dim * 2, 3)\n",
    "        self.fc2 = nn.Linear(gru_hidden_dim * 2, 3)\n",
    "        self.fc3 = nn.Linear(gru_hidden_dim * 2, 3)\n",
    "        self.fc4 = nn.Linear(gru_hidden_dim * 2, 3)\n",
    "        \n",
    "    def forward(self, x: torch.tensor, softmax: bool = False) -> torch.tensor:\n",
    "        embeddings = self.embedder(x)\n",
    "        _, outputs = self.encoder(embeddings)\n",
    "        logits1 = self.fc1(outputs.view(-1, self.gru_hidden_dim * 2))\n",
    "        logits2 = self.fc2(outputs.view(-1, self.gru_hidden_dim * 2))\n",
    "        logits3 = self.fc3(outputs.view(-1, self.gru_hidden_dim * 2))\n",
    "        logits4 = self.fc4(outputs.view(-1, self.gru_hidden_dim * 2))\n",
    "        \n",
    "        if len(x.shape) == 1:\n",
    "            logits1 = logits1.squeeze()\n",
    "            logits2 = logits2.squeeze()\n",
    "            logits3 = logits3.squeeze()\n",
    "            logits4 = logits4.squeeze()\n",
    "        \n",
    "        \n",
    "        return (logits1.softmax(axis=-1), logits2.softmax(axis=-1), \n",
    "               logits3.softmax(axis=-1), logits4.softmax(axis=-1)) if softmax else (\n",
    "            logits1, logits2, logits3, logits4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaselineGRU(\n",
       "  (embedder): Embedding(502, 5, padding_idx=0)\n",
       "  (encoder): GRU(5, 4, batch_first=True, bidirectional=True)\n",
       "  (fc1): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (fc2): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (fc3): Linear(in_features=8, out_features=3, bias=True)\n",
       "  (fc4): Linear(in_features=8, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 443,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BaselineGRU(len(vocab), 5, 4).to(\"cuda\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss, optimizer, n_epochs = 100):\n",
    "    # initialize tracker for minimum test loss\n",
    "    test_loss_min = np.Inf\n",
    "    for epoch in range(1, n_epochs):\n",
    "        train_loss = 0.0\n",
    "        test_loss = 0.0\n",
    "        \n",
    "        # training stage\n",
    "        model.train()\n",
    "        for X, Y in TRAIN_DATASET:\n",
    "            P = model(X, softmax=False)\n",
    "            \n",
    "            loss1 = loss(P[0].unsqueeze(dim=0), torch.tensor([Y[0]], dtype=torch.long, device=\"cuda\"))\n",
    "            loss2 = loss(P[1].unsqueeze(dim=0), torch.tensor([Y[1]], dtype=torch.long, device=\"cuda\"))\n",
    "            loss3 = loss(P[2].unsqueeze(dim=0), torch.tensor([Y[2]], dtype=torch.long, device=\"cuda\"))\n",
    "            loss4 = loss(P[3].unsqueeze(dim=0), torch.tensor([Y[3]], dtype=torch.long, device=\"cuda\"))\n",
    "            \n",
    "            loss_total = loss1 + loss2 + loss3 + loss4\n",
    "            \n",
    "            train_loss += loss_total.detach().item()\n",
    "            \n",
    "            loss_total.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        train_loss /= len(TRAIN_DATASET)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        accuracy1 = 0\n",
    "        accuracy2 = 0\n",
    "        accuracy3 = 0\n",
    "        accuracy4 = 0\n",
    "        \n",
    "        for X, Y in TEST_DATASET:\n",
    "            P = model(X, softmax=False)\n",
    "            \n",
    "            loss1 = loss(P[0].unsqueeze(dim=0), torch.tensor([Y[0]], dtype=torch.long, device=\"cuda\"))\n",
    "            loss2 = loss(P[1].unsqueeze(dim=0), torch.tensor([Y[1]], dtype=torch.long, device=\"cuda\"))\n",
    "            loss3 = loss(P[2].unsqueeze(dim=0), torch.tensor([Y[2]], dtype=torch.long, device=\"cuda\"))\n",
    "            loss4 = loss(P[3].unsqueeze(dim=0), torch.tensor([Y[3]], dtype=torch.long, device=\"cuda\"))\n",
    "            \n",
    "            test_loss += loss1.detach().item() + loss2.detach().item() + loss3.detach().item() + loss4.detach().item()\n",
    "            \n",
    "            accuracy1 += 1 if P[0].softmax(axis=-1).argmax().detach().item() == Y[0] else 0\n",
    "            accuracy2 += 1 if P[0].softmax(axis=-1).argmax().detach().item() == Y[1] else 0\n",
    "            accuracy3 += 1 if P[0].softmax(axis=-1).argmax().detach().item() == Y[2] else 0\n",
    "            accuracy4 += 1 if P[0].softmax(axis=-1).argmax().detach().item() == Y[3] else 0\n",
    "            \n",
    "        \n",
    "        test_loss /= len(TEST_DATASET)\n",
    "        accuracy1 /= len(TEST_DATASET)\n",
    "        accuracy2 /= len(TEST_DATASET)\n",
    "        accuracy3 /= len(TEST_DATASET)\n",
    "        accuracy4 /= len(TEST_DATASET)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if epoch % 5 == 0:\n",
    "            print(f\"{epoch:5d} - {train_loss:8.6f} {test_loss:8.6f} {accuracy1:10.6%} {accuracy2:10.6%} {accuracy3:10.6%} {accuracy4:10.6%}\")\n",
    "\n",
    "        if test_loss < test_loss_min:\n",
    "            torch.save(model, 'model.pt')\n",
    "            print('Testing loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            test_loss_min,\n",
    "            test_loss))\n",
    "            test_loss_min = test_loss\n",
    "    # return trained model\n",
    "    return model    \n",
    "                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing loss decreased (inf --> 3.972430).  Saving model ...\n",
      "Testing loss decreased (3.972430 --> 3.906049).  Saving model ...\n",
      "Testing loss decreased (3.906049 --> 3.850346).  Saving model ...\n",
      "Testing loss decreased (3.850346 --> 3.803603).  Saving model ...\n",
      "    5 - 3.904187 3.764029 51.366120% 52.459016% 49.180328% 47.540984%\n",
      "Testing loss decreased (3.803603 --> 3.764029).  Saving model ...\n",
      "Testing loss decreased (3.764029 --> 3.730601).  Saving model ...\n",
      "Testing loss decreased (3.730601 --> 3.702734).  Saving model ...\n",
      "Testing loss decreased (3.702734 --> 3.680078).  Saving model ...\n",
      "Testing loss decreased (3.680078 --> 3.662392).  Saving model ...\n",
      "   10 - 3.801156 3.649312 55.191257% 56.830601% 60.109290% 49.726776%\n",
      "Testing loss decreased (3.662392 --> 3.649312).  Saving model ...\n",
      "Testing loss decreased (3.649312 --> 3.640173).  Saving model ...\n",
      "Testing loss decreased (3.640173 --> 3.633980).  Saving model ...\n",
      "Testing loss decreased (3.633980 --> 3.629569).  Saving model ...\n",
      "Testing loss decreased (3.629569 --> 3.625907).  Saving model ...\n",
      "   15 - 3.787732 3.622349 57.377049% 55.737705% 63.387978% 50.819672%\n",
      "Testing loss decreased (3.625907 --> 3.622349).  Saving model ...\n",
      "Testing loss decreased (3.622349 --> 3.618701).  Saving model ...\n",
      "Testing loss decreased (3.618701 --> 3.615103).  Saving model ...\n",
      "Testing loss decreased (3.615103 --> 3.611862).  Saving model ...\n",
      "Testing loss decreased (3.611862 --> 3.609320).  Saving model ...\n",
      "   20 - 3.779885 3.607740 57.377049% 55.737705% 63.387978% 50.819672%\n",
      "Testing loss decreased (3.609320 --> 3.607740).  Saving model ...\n",
      "Testing loss decreased (3.607740 --> 3.607283).  Saving model ...\n",
      "   25 - 3.763627 3.617028 57.377049% 55.737705% 63.387978% 50.819672%\n",
      "   30 - 3.752998 3.626780 57.377049% 55.737705% 63.387978% 50.819672%\n",
      "   35 - 3.736890 3.611350 57.377049% 55.737705% 63.387978% 50.819672%\n",
      "Testing loss decreased (3.607283 --> 3.605038).  Saving model ...\n",
      "Testing loss decreased (3.605038 --> 3.602462).  Saving model ...\n",
      "Testing loss decreased (3.602462 --> 3.600371).  Saving model ...\n",
      "   40 - 3.717016 3.598781 57.377049% 55.737705% 63.387978% 50.819672%\n",
      "Testing loss decreased (3.600371 --> 3.598781).  Saving model ...\n",
      "Testing loss decreased (3.598781 --> 3.597642).  Saving model ...\n",
      "Testing loss decreased (3.597642 --> 3.596831).  Saving model ...\n",
      "Testing loss decreased (3.596831 --> 3.596170).  Saving model ...\n",
      "Testing loss decreased (3.596170 --> 3.595445).  Saving model ...\n",
      "   45 - 3.688868 3.594442 61.202186% 61.202186% 64.480874% 56.284153%\n",
      "Testing loss decreased (3.595445 --> 3.594442).  Saving model ...\n",
      "Testing loss decreased (3.594442 --> 3.592995).  Saving model ...\n",
      "Testing loss decreased (3.592995 --> 3.591045).  Saving model ...\n",
      "Testing loss decreased (3.591045 --> 3.588709).  Saving model ...\n",
      "Testing loss decreased (3.588709 --> 3.586279).  Saving model ...\n",
      "   50 - 3.648744 3.584169 55.737705% 57.923497% 58.469945% 50.819672%\n",
      "Testing loss decreased (3.586279 --> 3.584169).  Saving model ...\n",
      "Testing loss decreased (3.584169 --> 3.582800).  Saving model ...\n",
      "Testing loss decreased (3.582800 --> 3.582494).  Saving model ...\n",
      "   55 - 3.589471 3.588389 55.737705% 57.923497% 58.469945% 50.819672%\n",
      "   60 - 3.502080 3.617244 54.644809% 55.737705% 55.191257% 49.726776%\n",
      "   65 - 3.381876 3.704216 54.644809% 54.644809% 55.737705% 49.726776%\n",
      "   70 - 3.240546 3.857291 49.726776% 50.819672% 50.819672% 45.901639%\n",
      "   75 - 3.091477 4.036467 48.087432% 50.273224% 49.180328% 45.355191%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[446], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_GRU \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[445], line 22\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loss, optimizer, n_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     loss_total \u001b[38;5;241m=\u001b[39m loss1 \u001b[38;5;241m+\u001b[39m loss2 \u001b[38;5;241m+\u001b[39m loss3 \u001b[38;5;241m+\u001b[39m loss4\n\u001b[1;32m     20\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_total\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mloss_total\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/.anaconda/envs/rick/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.anaconda/envs/rick/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_GRU = train_model(model, loss_fn, optimizer, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
